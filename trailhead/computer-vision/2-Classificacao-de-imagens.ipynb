{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificação de Imagens\n",
    "O objetivo deste projeto é classificar fotos de algumas flores. Mais especificamente, a partir de um conjunto de dados do Kaggle, temos cinco tipos de flores: Margaridas, Dentes de Leão, Rosas, Girassóis e Tulipas, totalizando 4242 imagens.\n",
    "\n",
    "Além disso, este projeto pode ser visto também como um tutorial de Transfer Learning, transferência de aprendizado usando uma CNN (Rede neural convolucional). Há um guia passo-a-passo para realizar um classificador CNN usando o Keras com um modelo projetado por mim e outro usando o Transfer Learning.\n",
    "\n",
    "O que é Transfer Learning?\n",
    "\n",
    "É a reutilização de um modelo pré-treinado em um novo problema. Isto é, vou usar uma rede neural treinada em outro outro conjunto de dados, geralmente maior, para resolver um novo problema.\n",
    "\n",
    "Por que usar o Transfer Learning?\n",
    "\n",
    "Já que é difícil obter um conjunto de dados grande o suficiente, raramente CNNs são treinadados do zero;\n",
    "CNN muito profundas são muito caras para serem treinadas. Os modelos mais complexos podem demorar uma semana utilizando centenas de GPUs muito caras (Tesla V100 e cia);\n",
    "A maioria dos problemas envolvendo visão computacional usam conjuntos de dados aparentemente grandes (4000 - 20000 imagens), mas que ainda não são grandes o suficiente para se treinar adequadamente uma CNN. Assim, treinar estas redes com centenas de milhões de parametros quase sempre criam o problema do overfitting. Então, usamos o Transfer Learning para nos ajudar.\n",
    "\n",
    "Para este artigo, todos dados usados estão disponíveis no Kaggle:\n",
    "[Link](https://www.kaggle.com/alxmamaev/flowers-recognition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré-processamento de dados\n",
    "Como vamos fazer um tutorial prático sobre o Transfer Learning, é necessário um breve tratamento dos dados antes de realizar o treinamento da rede neural. Para tudo ficar bem claro, vou colocar bastante código no artigo e também, todo o código está disponível no GitHub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importando bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings('always') \n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "import seaborn as snsfrom \n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.optimizers import Nadam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Dropout, Flatten, Input, Dense\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import cv2 as cv\n",
    "import os \n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lendo e tratando os dados\n",
    "Depois de importar tudo que será utilizado, vamos ler as imagens de flores disponíveis. Na pasta “dandelion” (dente de leão), existiam arquivos que não eram imagens, então eu os deletei manualmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "daisy_dir = glob.glob(os.path.join('daisy/', '*'))\n",
    "dandelion_dir = glob.glob(os.path.join('dandelion/', '*'))\n",
    "rose_dir = glob.glob(os.path.join('rose/', '*'))\n",
    "sunflower_dir = glob.glob(os.path.join('sunflower/', '*'))\n",
    "tulip_dir = glob.glob(os.path.join('tulip/', '*'))\n",
    "X_path = daisy_dir + dandelion_dir + rose_dir + sunflower_dir + tulip_dir\n",
    "X = []\n",
    "for f in X_path:\n",
    "    X.append(np.array(cv.resize(cv.imread(f), (224,224), interpolation = cv.INTER_AREA))) \n",
    "X = np.array(X)\n",
    "X = X / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada imagem tem um tamanho diferente, e como sabemos, para entrar numa rede neural todas amostras precisam ter as mesmas dimensões, no caso, escolhi 224x244, isto porque as arquiteturas mais comuns com pesos disponíveis para o Transfer Learning usam estas mesmas dimensões.\n",
    "\n",
    "Para normalizar os dados, simplesmente efetuamos uma divisão por 255 já que os valores de cada pixel são RGB variando de 0 à 255.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definindo as labels\n",
    "A entrada de nossa rede neural já está pronta, agora devemos definir os rótulos (labels) para o treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_daisy = np.zeros(len(daisy_dir))\n",
    "l_dandelion = np.ones(len(dandelion_dir))\n",
    "l_rose = 2*np.ones(len(rose_dir))\n",
    "l_sunflower = 3*np.ones(len(sunflower_dir))\n",
    "l_tulip = 4*np.ones(len(tulip_dir))\n",
    "y = np.concatenate((l_daisy, l_dandelion, l_rose, l_sunflower, l_tulip))\n",
    "y = to_categorical(y, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Desta forma, nossas labels estão todas codificadas com o One-Hot-Encoding._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dando uma olhada em nossas imagens\n",
    "Particularmente, sempre gosto de dar uma olhada no conjunto de dados antes de sair classificando tudo. Então resolvi mostrar algumas imagens do nosso conjunto. Olhando mais detalhadamente todo o conjunto, notei que existem algumas (menos de 10) imagens com labels erradas e que nem flores são. Preferi não deletar estas imagens para manter o experimento com o conjunto original do Kaggle.\n",
    "![IMAGE 0](https://miro.medium.com/max/875/1*Qe366PBEGDQ2YQ_5_qw2bQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separando treinamento e validação\n",
    "Agora vamos separar o nosso conjunto e duas partes. Uma para o treinamento e outra para a validação. Idealmente, deveríamos separar isto em três partes, para avaliarmos o desempenho do classificar em um conjunto nunca visto nem para a validação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Para evitar problemas de overfitting, usei o ImageDataGenerator._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "        zoom_range = 0.1, # Aleatory zoom\n",
    "        rotation_range= 15, \n",
    "        width_shift_range=0.1,  # horizontal shift\n",
    "        height_shift_range=0.1,  # vertical shift\n",
    "        horizontal_flip=True,  \n",
    "        vertical_flip=True)\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN do zero\n",
    "Agora sim, vamos fazer uma CNN simples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input((224,224,3))\n",
    "conv1 = Conv2D(64, (5,5), padding='valid', activation= 'relu')(inp)\n",
    "conv1 = MaxPooling2D(pool_size=(2,2))(conv1)\n",
    "conv1 = BatchNormalization()(conv1)\n",
    "conv2 = Conv2D(96, (4,4), padding='valid', activation= 'relu')(conv1)\n",
    "conv2 = MaxPooling2D(pool_size=(2,2))(conv2)\n",
    "conv2 = BatchNormalization()(conv2)\n",
    "conv3 = Conv2D(128, (3,3), padding='valid', activation= 'relu')(conv2)\n",
    "conv3 = MaxPooling2D(pool_size=(2,2))(conv3)\n",
    "conv3 = BatchNormalization()(conv3)\n",
    "conv4 = Conv2D(256, (3,3), padding='valid', activation= 'relu')(conv3)\n",
    "conv4 = MaxPooling2D(pool_size=(2,2))(conv4)\n",
    "conv4 = BatchNormalization()(conv4)\n",
    "flat = Flatten()(conv4)\n",
    "dense1 = Dense(512, activation= 'relu')(flat)\n",
    "dense1 = Dropout(0.5)(dense1)\n",
    "dense2 = Dense(64, activation= 'relu')(dense1)\n",
    "dense2 = Dropout(0.1)(dense2)\n",
    "out = Dense(5, activation = 'softmax')(dense2)\n",
    "model = Model(inp, out)\n",
    "model.compile(optimizer = Nadam(lr = 0.0001) , loss = 'categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E finalmente, treiná-la:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "history = model.fit(X_train, y_train, batch_size = 32, epochs = 50, initial_epoch = 0, validation_data = (X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![IMAGE 1](https://miro.medium.com/max/792/1*ymJTsl4fmxw21CHwqX-GoQ.png)\n",
    "![IMAGE 2](https://miro.medium.com/max/778/1*rYOWtO4dqGdaBGNcR8jzsA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É fácil ver o problema de overfitting nos gráficos acima. A acurácia de treinamento está próxima de 1 enquanto, no teste próxima de 0.7.\n",
    "\n",
    "Existem várias formas de reduzir o overfitting, como a regularização BatchNormalization, Dropout e também o aumento do conjunto dados. Vamos nos focar nesta ideia de aumentar o conjunto de dados. Sabendo que não temos mais imagens de flores disponíveis, vamos usar o Transfer Learning como uma forma de reduzir o overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning\n",
    "Vamos utilizar o modelo da VGG-16, com pesos treinados na ImageNet, este conjunto de dados é composto por mais de 14 milhões de imagens classificadas em mais de 20000 classes. Sendo assim, é um conjunto substancialmente maior que nosso conjunto de flores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = keras.applications.VGG16(input_shape=(224,224,3), include_top = False, weights= 'imagenet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desta forma, temos um modelo composto apenas pelas camadas convolucionais pré-treinadas da VGG-16. E agora?\n",
    "\n",
    "Tendo em mente que os recursos da CNN são mais genéricos (ex. detector de borda) nas camadas iniciais e mais específicos (ex. pétalas) do conjunto de dados originais nas camadas posteriores, aqui estão algumas regras comuns para navegar pelos 4 principais cenários:\n",
    "\n",
    "![IMAGE 4](https://miro.medium.com/max/875/1*BwCvo9RY5YzKkbtx1vIwZg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conjunto de dados pequeno e diferente do modelo pré-treinado\n",
    "- Adiciona-se camadas densamente conectas depois das camadas convolucionais;\n",
    "- Congelamos os pesos das primeiras camadas convolucionais;\n",
    "- Treina-se a rede neural.\n",
    "### Conjunto de dados grande e diferente do modelo pré-treinado\n",
    "- Adiciona-se camadas densamente conectas depois das camadas convolucionais;\n",
    "- Treina-se a rede neural usando os pesos da ImageNet como valores inciais.\n",
    "### Conjunto de dados grande e similar ao do modelo pré-treinado\n",
    "- Adiciona-se camadas densamente conectas depois das camadas convolucionais;\n",
    "- Pode-se treinar todo o modelo ou congelar os pesos das primeiras camadas convolucionais.\n",
    "### Conjunto de dados pequeno e similar ao modelo pré-treinado\n",
    "- Adiciona-se camadas densamente conectas depois das camadas convolucionais;\n",
    "- Congelar os pesos das primeiras camadas convolucionais ou todas camadas convolucionais.\n",
    "\n",
    "Então, em qual dos casos nosso problema de reconhecimento de flores se enquadra? Lembrando que o conjunto de dados possui 4242 amostras de cinco possíveis classes.\n",
    "\n",
    "Perto do conjunto da ImageNet, nosso conjunto de flores é muito pequeno, então vamos adicionar camadas densamente conectadas e congelar o treinamento de todas as camadas convolucionais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = vgg.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(3078,activation='relu')(x) \n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(256,activation='relu')(x) \n",
    "x = Dropout(0.2)(x)\n",
    "out = Dense(5,activation='softmax')(x)\n",
    "tf_model=Model(inputs=vgg.input,outputs=out)\n",
    "for layer in tf_model.layers[:20]:\n",
    "    layer.trainable=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, vamos treinar nosso modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = tf_model.fit(X_train, y_train, batch_size = 1, epochs = 30, initial_epoch = 0, validation_data = (X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![IMAGE 5](https://miro.medium.com/max/778/1*weUj3VcHHRDU5TZqe8xJow.png)\n",
    "![IMAGE 6](https://miro.medium.com/max/778/1*je___X113I9bYz6iuSxZiA.png)\n",
    "\n",
    "Claramente, a acurácia melhorou bastante (85% no conjunto de validação). No entanto, ainda existe o problema de overfitting. Talvez treinando todas camadas convolucionais o modelo teria um desempenho melhor, mas isto fica para você treinar!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classificando alguns exemplos\n",
    "Vamos classificar alguns exemplos de flores para ver onde é que nosso modelo está errando mais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = tf_model.predict(X_val)\n",
    "pred = np.argmax(pred, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![IMAGE 7](https://miro.medium.com/max/875/1*V6zeCIwG2oLkwyD3HLFkHQ.png)\n",
    "\n",
    "Podemos ver que as fotos dentre estes exemplos, algumas flores não estão em seu formato mais comum ou estão em grande quantidade ou em baixa resolução ou ainda existe mais de um tipo de flor na foto. Todos estes fatores são impactantes na classificação de nosso modelo neural.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusão\n",
    "O Transfer Learning definitivamente será um dos principais impulsionadores para o Machine Learning e para o sucesso do Deep Learning na adoção mainstream na indústria. Através dele é possível acelerar e melhorar um modelo profundo com facilidade, além de também reduzir os custos computacionais atrelados ao treinamento.\n",
    "\n",
    "Existem diversas outras arquiteturas pré-treinadas como:\n",
    "\n",
    "- Xception\n",
    "- VGG-19\n",
    "- ResNet, ResNetV2, ResNeXt\n",
    "- InceptionV3\n",
    "- MobileNet, MobileNetV2\n",
    "- DenseNet\n",
    "- NASNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teste\n",
    "\n",
    "1. A utilização de redes neurais convolucionais em visão computacional destaca-se por:\n",
    "   > Resposta: Reconhecimento de objetos"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3294317547875cb551fdfa992645c6d6ebaab3226b1a2340bf1f853a0a794e92"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('cognitive-computing')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
